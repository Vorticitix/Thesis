{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/onno/Thesis/Scripts')\n",
    "import my_tools\n",
    "from my_tools import plot_dic, file_dic\n",
    "%matplotlib qt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporate-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path\n",
    "path = '/media/onno/Algemeen/Thesis/GFS_T850/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "inner-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8f9e5ccf4a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0monset_fcst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit_dayz_fcst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mend_fcst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit_dayz_fcst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mduration_fcst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len_boolean_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit_dayz_fcst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0monset_rean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit_dayz_rean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/Scripts/my_tools.py\u001b[0m in \u001b[0;36mmax_len_boolean_array\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_2D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midx_2D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m#caluclate longest consecutive series of true Values. Add one because the last day counts as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "coordinatez = [\n",
    "(54,46,6,14),#Germany\n",
    "(44,36,352,360), #Spain\n",
    "(54,46,26,34), #Ukraine\n",
    "(58,50,352,360), #UK\n",
    "(42,34,28,36), #Turkey\n",
    "(68,60,22,30), #Finland\n",
    "(66,58,6,14), #Norway/Sweden\n",
    "(60,52,46,54)] #Russia Samara/Kazan region\n",
    "#loading GFS and ERA5 data for specific box \n",
    "\n",
    "seasonz = {'MAM':[3,4,5],\n",
    "          'JJA':[6,7,8],\n",
    "          'SON':[9,10,11],\n",
    "          'DJF':[12,1,2]}\n",
    "\n",
    "fcst_modelz = ['GFS','ERA5RF']\n",
    "for fcst_model in fcst_modelz:\n",
    "    for lat_0,lat_1,lon_0,lon_1 in coordinatez:\n",
    "        file_fcst = file_dic['T850_grid'][fcst_model].format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_rean = file_dic['T850_grid']['ERA5'].format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_clim_p90 = 'era51_mars_t850_79-19_24hourly_90p_lon_{}_{}_lat_{}_{}_SMOOTHED.nc'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_clim_p10 = 'era51_mars_t850_79-19_24hourly_10p_lon_{}_{}_lat_{}_{}_SMOOTHED.nc'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        fcst = xr.open_dataset(path+file_fcst,decode_times=False).squeeze()\n",
    "        #set fcst time data to datetime object\n",
    "        init_time_fcst = pd.Timestamp('1984-12-01')\n",
    "        fcst['time'] = [pd.Timedelta(i-fcst.time.values[0],'hours')+init_time_fcst for i in fcst.time.values]\n",
    "        fcst['lead'] = fcst['lead']//24\n",
    "        #group forecast by lead time and take daily means\n",
    "        fcst = fcst.sel(lead=slice(0,9)).groupby('lead').mean()\n",
    "        rean = xr.open_dataset(path+file_rean).squeeze()\n",
    "        #load percentile climatology. For convenience I set the data to all days in the leap year 2016. That has no further effect on the data\n",
    "        clim_p90 = xr.open_dataset(path+file_clim_p90).squeeze()\n",
    "        clim_p90['time']=pd.date_range('2016-01-01',\"2016-12-31\")\n",
    "        clim_p10 = xr.open_dataset(path+file_clim_p10).squeeze()\n",
    "        clim_p10['time']=pd.date_range('2016-01-01',\"2016-12-31\")\n",
    "        #load all heat extremes\n",
    "        file_pers_hw = 'persistent_hw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_pers_cw = 'persistent_cw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_short_hw = 'short_hw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_short_cw = 'short_cw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_independent_short_hw = 'independent_short_hw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        file_independent_short_cw = 'independent_short_cw_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_0-2,lat_1)\n",
    "        pers_hw = np.load(path+file_pers_hw)\n",
    "        pers_cw = np.load(path+file_pers_cw)\n",
    "        short_hw = np.load(path+file_short_hw)\n",
    "        short_cw = np.load(path+file_short_cw)\n",
    "        #Load independt short temp extremes\n",
    "        independent_short_hw = np.load(path+file_independent_short_hw)\n",
    "        independent_short_cw = np.load(path+file_independent_short_cw)\n",
    "        temp_extremez = [pers_hw,short_hw,pers_cw,short_cw]\n",
    "        columnz = ['persistent_hw','short_hw','persistent_cw','short_cw',]\n",
    "        #loop over lead dayz\n",
    "        lead_dayz = [3,5]\n",
    "        for lead_day in lead_dayz:\n",
    "            #loop over temperature extreme types\n",
    "            for j,temp_extreme in enumerate(temp_extremez):\n",
    "                df  = pd.DataFrame(index=temp_extreme[:,0],columns=['onset_error','end_error','duration_error'])\n",
    "                column = columnz[j]\n",
    "                #loop over all dates for specific temperature extreme\n",
    "                for i,date in enumerate(temp_extreme[:,0]):\n",
    "                    #if short heatwave is not independent skip date\n",
    "                    if (j==1)and(independent_short_hw[i]==False):\n",
    "                        continue\n",
    "                        \n",
    "                    #if short coldwave is not independent skip date\n",
    "                    if (j==3)and(independent_short_cw[i]==False):\n",
    "                        continue\n",
    "                    #if forecast initialization date is before the day that data is available skip date\n",
    "                    if date<pd.Timestamp('1984-12-01')+pd.Timedelta(lead_day,'days'):\n",
    "                        continue\n",
    "                    #set forecast initizialition date\n",
    "                    begin_date = date - pd.Timedelta(lead_day,'days')\n",
    "                    #three dates are missing from ERA4 forecast, skip dates\n",
    "                    if np.isin(begin_date,[pd.Timestamp('2014-10-29'),\n",
    "                                           pd.Timestamp('2014-10-30'),\n",
    "                                           pd.Timestamp('2014-10-31')]):\n",
    "                        continue\n",
    "                    #Set end_date of forecast\n",
    "                    end_date = date + pd.Timedelta(9-lead_day,'days')\n",
    "                    #load 10 day temperature data for specific event for reanalysis and forecast\n",
    "                    rean_sub = rean.sel(time=slice(begin_date,end_date)).load()\n",
    "                    fcst_sub = fcst.sel(time=begin_date).load()\n",
    "                    #load subsequent daily climate dates. the year 2016 is used as reference year but has no meaning.\n",
    "                    timez = [pd.Timestamp('2016-{:02d}-{:02d}'.format(i.month,i.day)) for i in pd.date_range(begin_date,end_date)]\n",
    "                    #for heatwaves load 90th percentile data. \n",
    "                    if j<2:\n",
    "                        clim_p90_sub = clim_p90.sel(time=timez).load()\n",
    "                        rean_bool = rean_sub.t>clim_p90_sub.t.values\n",
    "                        fcst_bool = fcst_sub.t>clim_p90_sub.t.values\n",
    "                    #for coldwaves load 10th percentile data\n",
    "                    else:\n",
    "                        clim_p10_sub = clim_p10.sel(time=timez).load()\n",
    "                        rean_bool = rean_sub.t<clim_p10_sub.t.values\n",
    "                        fcst_bool = fcst_sub.t<clim_p10_sub.t.values\n",
    "                    #Calculate onset day, end day and duration of event in reanalysis and forecast\n",
    "                    try:\n",
    "                        hit_dayz_fcst = np.squeeze(np.where(fcst_bool))\n",
    "                        #skip the first 2 days in forecast because these may cause a hit that has nothing to do with the actual event.\n",
    "                        hit_dayz_fcst = hit_dayz_fcst[hit_dayz_fcst>=2]\n",
    "                        hit_dayz_rean = np.squeeze(np.where(rean_bool))\n",
    "                        hit_dayz_rean = hit_dayz_rean[hit_dayz_rean>=lead_day]\n",
    "                        \n",
    "                        onset_fcst = hit_dayz_fcst[0]\n",
    "                        end_fcst = hit_dayz_fcst[-1]\n",
    "                        duration_fcst = my_tools.max_len_boolean_array(fcst_bool)\n",
    "                        \n",
    "                        onset_rean = hit_dayz_rean[0]\n",
    "                        end_rean = hit_dayz_rean[-1]\n",
    "                        duration_rean = my_tools.max_len_boolean_array(rean_bool)\n",
    "                        \n",
    "                        #Calculate differences between forecast and reanalysis\n",
    "                        onset_error = onset_fcst - onset_rean\n",
    "                        end_error = end_fcst - end_rean\n",
    "                        duration_error = duration_fcst - duration_rean\n",
    "                        \n",
    "                        #put errors in dataframe\n",
    "                        df.loc[date,'onset_error'] = onset_error\n",
    "                        df.loc[date,'end_error'] = end_error\n",
    "                        df.loc[date,'duration_error'] = duration_error\n",
    "                        sys.exit()\n",
    "                    #sometimes tehre is not a single hit in the forecast which will yield an IndexError. Put values for this to nan   \n",
    "                    except IndexError:\n",
    "                        df.loc[date,'onset_error'] = np.nan\n",
    "                        df.loc[date,'end_error'] = np.nan\n",
    "                        df.loc[date,'end_error'] = np.nan\n",
    "                #split dataframe for every season and save\n",
    "                for season in seasonz:\n",
    "                    df_season = df[np.isin(df.index.month,seasonz[season])]\n",
    "                    df_season.to_csv(path+'/duration_error/duration_error_{}_lon_{}_{}_lat_{}_{}_{}_{}_lead_day_{}.txt'.format(column,lon_0,lon_1,\n",
    "                                                                                                    lat_0,lat_1,season,fcst_model,lead_day))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-chick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
