{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/onno/Thesis/Scripts')\n",
    "import my_tools\n",
    "from my_tools import plot_dic, file_dic\n",
    "%matplotlib qt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path\n",
    "path = '/media/onno/Volume/GFS_T850/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GSS for every single vent on lead days 1, 3, 5, 7 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinatez = [\n",
    "(62,54,0,8),#Germany\n",
    "(54,46,0,8), #Spain\n",
    "(46,38,0,8), #Ukraine\n",
    "(62,54,8,16),#Germany\n",
    "(54,46,8,16), #Spain\n",
    "(46,38,8,16), #Ukraine\n",
    "(62,54,16,24),#Germany\n",
    "(54,46,16,24), #Spain\n",
    "(46,38,16,24),\n",
    "]#Ukraine\n",
    "seasonz = {'MAM':[3,4,5],\n",
    "          'JJA':[6,7,8],\n",
    "          'SON':[9,10,11],\n",
    "          'DJF':[12,1,2]}\n",
    "\n",
    "#Choose Forecast model: GFS or ERA5RF\n",
    "fcst_modelz = ['ERA5RF']\n",
    "for fcst_model in fcst_modelz:\n",
    "    for lat_1,lat_0,lon_0,lon_1 in coordinatez:\n",
    "        Path(\"/media/onno/Volume/GFS_T850/anom_files/lon_{}_{}_lat_{}_{}/\"\\\n",
    "             .format(lon_0,lon_1,lat_1,lat_0)).mkdir(parents=True, exist_ok=True)\n",
    "#         file_fcst = file_dic['T850_grid'][fcst_model].format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_fcst = 'era5rf_fldmean_t850_0-240h_24hourly_2x2nh_jan79-dec19_lon_{}_{}_lat_{}_{}_ALL_NEW.nc'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_rean = file_dic['T850_grid']['ERA5'].format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_clim_p90 = 'era5_mars_t850_79-19_24hourly_90p_lon_{}_{}_lat_{}_{}_SMOOTHED_ALL.nc'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_clim_p50 = 'era5_mars_t850_79-19_24hourly_50p_lon_{}_{}_lat_{}_{}_SMOOTHED_ALL.nc'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_clim_p10 = 'era5_mars_t850_79-19_24hourly_10p_lon_{}_{}_lat_{}_{}_SMOOTHED_ALL.nc'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        fcst = xr.open_dataset(path+file_fcst,decode_times=False).squeeze()\n",
    "        #set fcst time data to datetime object\n",
    "        init_time_fcst = pd.Timestamp('1979-01-01')\n",
    "        fcst['time'] = [pd.Timedelta(i-fcst.time.values[0],'hours')+init_time_fcst for i in fcst.time.values]\n",
    "        fcst['lead'] = fcst['lead']//24\n",
    "        #group forecast by lead time and take daily means\n",
    "        fcst = fcst.sel(lead=slice(0,9)).groupby('lead').mean()\n",
    "        rean = xr.open_dataset(path+file_rean).squeeze()\n",
    "        if int(rean.time[0].dt.hour)!=0:\n",
    "            rean = rean.assign_coords({'time':rean.time.values - pd.Timedelta(int(rean.time[0].dt.hour),'h')})\n",
    "        #load percentile climatology. For convenience I set the data to all days in the leap year 2016. That has no further effect on the data\n",
    "        clim_p90 = xr.open_dataset(path+file_clim_p90).squeeze()\n",
    "        clim_p90['time']=pd.date_range('2016-01-01',\"2016-12-31\")\n",
    "        clim_p50 = xr.open_dataset(path+file_clim_p50).squeeze()\n",
    "        clim_p50['time']=pd.date_range('2016-01-01',\"2016-12-31\")\n",
    "        clim_p10 = xr.open_dataset(path+file_clim_p10).squeeze()\n",
    "        clim_p10['time']=pd.date_range('2016-01-01',\"2016-12-31\")\n",
    "        #load all heat extremes\n",
    "        file_pers_hw = 'dates/pw_lon_{}_{}_lat_{}_{}_new.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "        file_pers_cw = 'dates/pc_lon_{}_{}_lat_{}_{}_new.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "        file_short_hw = 'short_heatwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_short_cw = 'short_coldwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_independent_short_hw = 'independent_short_heatwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        file_independent_short_cw = 'independent_short_coldwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1-2,lat_1-2,lat_0)\n",
    "        pers_hw = np.load(path+file_pers_hw,allow_pickle=True)\n",
    "        pers_cw = np.load(path+file_pers_cw,allow_pickle=True)\n",
    "        #short_hw = np.load(path+file_short_hw)\n",
    "        #short_cw = np.load(path+file_short_cw)\n",
    "        #independent_short_hw = np.load(path+file_independent_short_hw)\n",
    "        #independent_short_cw = np.load(path+file_independent_short_cw)\n",
    "        #temp_extremez = [pers_hw,short_hw,pers_cw,short_cw]\n",
    "        temp_extremez = [pers_hw,pers_cw]\n",
    "        columnz = ['persistent_hw','persistent_cw']\n",
    "        index = pd.date_range(pd.Timestamp('1979-01-01'),pd.Timestamp('2019-12-31'))\n",
    "        lead_dayz = [3,5,7]\n",
    "        for lead_day in lead_dayz:\n",
    "            df_GSS = pd.DataFrame(index=index,columns=columnz,dtype=float)\n",
    "            df_HK = pd.DataFrame(index=index,columns=columnz,dtype=float)\n",
    "            for j,temp_extreme in enumerate(temp_extremez):\n",
    "                column = columnz[j]\n",
    "                df_contingency = pd.DataFrame(columns=['hits','miss','false_alarm','non_event'])\n",
    "                for i,date in enumerate(temp_extreme[:,0]):\n",
    "#                     if (j==1)and(independent_short_hw[i]==False):\n",
    "#                         continue\n",
    "#                     if (j==3)and(independent_short_cw[i]==False):\n",
    "#                         continue\n",
    "                    date = pd.Timestamp(date)\n",
    "                    if date<pd.Timestamp('1979-01-01')+pd.Timedelta(lead_day,'days'):\n",
    "                        continue\n",
    "                    begin_date = date - pd.Timedelta(lead_day,'days')\n",
    "                    if np.isin(begin_date,[pd.Timestamp('2014-10-29'),\n",
    "                                           pd.Timestamp('2014-10-30'),\n",
    "                                           pd.Timestamp('2014-10-31')]):\n",
    "                        continue\n",
    "                    end_date = date + pd.Timedelta(9-lead_day,'days')\n",
    "                    rean_sub = rean.sel(time=slice(begin_date,end_date)).load()\n",
    "                    fcst_sub = fcst.sel(time=begin_date).load()\n",
    "\n",
    "                    timez = [pd.Timestamp('2016-{:02d}-{:02d}'.format(i.month,i.day)) for i in pd.date_range(begin_date,end_date)]\n",
    "                    clim_p50_sub = clim_p50.sel(time=timez).load()\n",
    "                    if j==0:\n",
    "                        clim_p90_sub = clim_p90.sel(time=timez).load()\n",
    "                        rean_sub_anom = rean_sub.t - clim_p90_sub.t.values\n",
    "                        fcst_sub_anom = fcst_sub.t - clim_p90_sub.t.values\n",
    "                        assert((rean_sub_anom.sel(time=date)>0).all())\n",
    "                        rean_sub_anom.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/rean_{}_{:02d}_{:02d}_lead_day_{}_pw_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        fcst_sub_anom.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/fcst_{}_{:02d}_{:02d}_lead_day_{}_pw_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        rean_sub_anom_median = rean_sub.t - clim_p50_sub.t.values\n",
    "                        fcst_sub_anom_median = fcst_sub.t - clim_p50_sub.t.values\n",
    "                        rean_sub_anom_median.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/rean_{}_{:02d}_{:02d}_lead_day_{}_pw_50p_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        fcst_sub_anom_median.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/fcst_{}_{:02d}_{:02d}_lead_day_{}_pw_50p_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        rean_bool = rean_sub.t>clim_p90_sub.t.values\n",
    "                        fcst_bool = fcst_sub.t>clim_p90_sub.t.values\n",
    "                    else:\n",
    "                        clim_p10_sub = clim_p10.sel(time=timez).load()\n",
    "                        rean_sub_anom = rean_sub.t - clim_p10_sub.t.values\n",
    "                        fcst_sub_anom = fcst_sub.t - clim_p10_sub.t.values\n",
    "                        assert((rean_sub_anom.sel(time=date)<0).all())\n",
    "                        rean_sub_anom.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/rean_{}_{:02d}_{:02d}_lead_day_{}_cw_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        fcst_sub_anom.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/fcst_{}_{:02d}_{:02d}_lead_day_{}_cw_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        rean_sub_anom_median = rean_sub.t - clim_p50_sub.t.values\n",
    "                        fcst_sub_anom_median = fcst_sub.t - clim_p50_sub.t.values\n",
    "                        rean_sub_anom_median.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/rean_{}_{:02d}_{:02d}_lead_day_{}_cw_50p_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        fcst_sub_anom_median.to_netcdf(path+'anom_files/lon_{}_{}_lat_{}_{}/fcst_{}_{:02d}_{:02d}_lead_day_{}_cw_50p_new.nc'.format(lon_0,lon_1,lat_1,lat_0,\n",
    "                                                                                                    date.year,date.month,date.day,lead_day))\n",
    "                        rean_bool = rean_sub.t<clim_p10_sub.t.values\n",
    "                        fcst_bool = fcst_sub.t<clim_p10_sub.t.values\n",
    "                        \n",
    "                    hits = (rean_bool & fcst_bool.values).astype(np.int64).sum()\n",
    "                    miss = ((~fcst_bool)&(rean_bool.values)).astype(np.int64).sum()\n",
    "                    false_alarm = ((fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()\n",
    "                    non_event = ((~fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()            \n",
    "                    GSS = my_tools.gilbert_skill_score(hits,miss,false_alarm,non_event)\n",
    "                    HK = my_tools.hansen_kuiper_discrimant(hits,miss,false_alarm,non_event)\n",
    "                    df_GSS.loc[date][column]=GSS\n",
    "                    df_HK.loc[date][column]=HK\n",
    "#                     fig,ax = plt.subplots(figsize=(12,12))\n",
    "#                     ax.plot(rean_sub.time,fcst_sub.t,label='fcst',marker='o')\n",
    "#                     ax.plot(rean_sub.time,rean_sub.t,label='rean',marker='o')\n",
    "#                     ax.plot(rean_sub.time,clim_p90_sub.t,color='r')\n",
    "#                     ax.legend()\n",
    "#                     ax.set_title('GSS = {:.2f} HK = {:.2f}'.format(float(GSS),float(HK)))\n",
    "#                     fig.savefig(path+'GSS/figures/ind_events/{}_{:02d}_{:02d}_warm.png'.format(date.year,\n",
    "#                                                                                          date.month,\n",
    "#                                                                                          date.day))\n",
    "            df_GSS = df_GSS.dropna(how='all')\n",
    "            df_HK = df_HK.dropna(how='all')\n",
    "\n",
    "            df_GSS.to_csv(path+'GSS/GSS_lead_day_{}_lon_{}_{}_lat_{}_{}_{}_new.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                             lat_0,lat_1,fcst_model))\n",
    "            df_HK.to_csv(path+'HK/HK_lead_day_{}_lon_{}_{}_lat_{}_{}_{}_new.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                             lat_0,lat_1,fcst_model))\n",
    "            index_season = ['DJF','MAM','JJA','SON']\n",
    "            df_season_mean_GSS = pd.DataFrame(index=index_season,columns=['persistent_hw','persistent_cw'])\n",
    "            df_season_mean_HK = pd.DataFrame(index=index_season,columns=['persistent_hw','persistent_cw'])\n",
    "            for season in index_season:\n",
    "                df_season_GSS = df_GSS[np.isin(df_GSS.index.month,seasonz[season])].mean()\n",
    "                df_season_mean_GSS.loc[season]=df_season_GSS.values\n",
    "                df_season_HK = df_HK[np.isin(df_HK.index.month,seasonz[season])].mean()\n",
    "                df_season_mean_HK.loc[season]=df_season_HK.values\n",
    "#             df_month_mean = df.groupby(df.index.month).mean()\n",
    "#             df_month_mean.index = month_to_season_lu\n",
    "#             df_season_mean = df_month_mean.groupby(df_month_mean.index).mean()\n",
    "            \n",
    "#             JJA,MAM = a.iloc[1].copy(),a.iloc[2].copy()\n",
    "#             df_season_mean.iloc[1]=MAM;df_season_mean.iloc[2]=JJA\n",
    "#             df_season_mean.index=index_month\n",
    "            df_season_mean_GSS.to_csv(path+'GSS/Monthly_mean_GSS_lead_day_{}_lon_{}_{}_lat_{}_{}_{}_new.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                                       lat_0,lat_1,fcst_model))\n",
    "            df_season_mean_HK.to_csv(path+'HK/Monthly_mean_HK_lead_day_{}_lon_{}_{}_lat_{}_{}_{}_new.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                                       lat_0,lat_1,fcst_model))\n",
    "        #     df_month = df.groupby(df.index.month,dropna=True).mean()\n",
    "        #     df_month.to_csv(path+'GSS/Gilbert_Skill_Score_Monthly_Mean_lead_day_{}.csv'.format(lead_day))\n",
    "        df_count = df_GSS.groupby(df_GSS.index.month,dropna=True).count()\n",
    "        df_count.to_csv(path+'GSS/Count_lon_{}_{}_lat_{}_{}_new.csv'.format(lon_0,lon_1,lat_0,lat_1))\n",
    "        df_season_count = pd.DataFrame(index=index_season,columns=['persistent_hw','persistent_cw'])\n",
    "        for season in index_season:\n",
    "            df_count_season = df_count[np.isin(df_count.index,seasonz[season])].sum()\n",
    "            df_season_count.loc[season]=df_count_season.values\n",
    "        df_season_count.to_csv(path+'GSS/Count_season_lon_{}_{}_lat_{}_{}_new.csv'.format(lon_0,lon_1,lat_0,lat_1))\n",
    "#     df.to_csv(path+'GSS/GSS_Temp_Extremes_Seasonally_Grouped_lead_day_{}.csv'.format(lead_day))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General monthly Gilbert Skill Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put climatologies in list to loop over them\n",
    "climz = [clim_p90,clim_p10]\n",
    "#set and select lead day. Value must range between 0 and 9\n",
    "lead_dayz = [1,3,5,7,9]\n",
    "#make reanalysis and forecast of equal length\n",
    "df_warm = pd.DataFrame(index=np.arange(1,13),columns=['day_1','day_3','day_5','day_7','day_9'])\n",
    "df_cold = pd.DataFrame(index=np.arange(1,13),columns=['day_1','day_3','day_5','day_7','day_9'])\n",
    "#loop over heat and cold extremes\n",
    "columnz = ['day_1','day_3','day_5','day_7','day_9']\n",
    "for lead_day in lead_dayz:\n",
    "    fcst_lead = fcst.sel(lead=lead_day)\n",
    "    #convert time of forecast to initial time + forecast lead time\n",
    "    fcst_lead['time'] = fcst.time + pd.Timedelta(lead_day,'days')\n",
    "    rean_lead = rean.sel(time=slice(pd.Timestamp('1984-12-{:02d}'.format(lead_day+1)),\n",
    "                             pd.Timestamp('2019-11-30')))\n",
    "    fcst_lead = fcst_lead.sel(time=slice(pd.Timestamp('1984-12-{:02d}'.format(lead_day+1)),\n",
    "                             pd.Timestamp('2019-11-30')))\n",
    "    for j,clim in enumerate(climz):\n",
    "        column = columnz[j]\n",
    "        #calculate anomalies from climatoogy: see my_tools.py for more documentation\n",
    "        rean_anom = my_tools.calculate_anomalies(rean_lead,clim)    \n",
    "        fcst_anom = my_tools.calculate_anomalies(fcst_lead,clim)\n",
    "\n",
    "        if j ==0:\n",
    "            #convert anomalies to boolean array. If temperature is extreme than value is converted to True\n",
    "            rean_bool = rean_anom.t>0\n",
    "            fcst_bool = fcst_anom.t>0\n",
    "            #fill in contingency table and group all values by month\n",
    "            hits = (rean_bool & fcst_bool).astype(np.int64).groupby('time.month').sum()\n",
    "            miss = ((~fcst_bool)&(rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            false_alarm = ((fcst_bool)&(~rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            non_event = ((~fcst_bool)&(~rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            #caluclate Gilbert Skill Score. See my_tools.py for more documentation\n",
    "            GSS = my_tools.gilbert_skill_score(hits,miss,false_alarm,non_event)\n",
    "            #Write result to csv file\n",
    "            df_warm['day_{}'.format(lead_day)]=pd.Series(GSS,index=np.arange(1,13))\n",
    "        else:\n",
    "            #convert anomalies to boolean array. If temperature is extreme than value is converted to True\n",
    "            rean_bool = rean_anom.t<0\n",
    "            fcst_bool = fcst_anom.t<0\n",
    "            #fill in contingency table and group all values by month\n",
    "            hits = (rean_bool & fcst_bool).astype(np.int64).groupby('time.month').sum()\n",
    "            miss = ((~fcst_bool)&(rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            false_alarm = ((fcst_bool)&(~rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            non_event = ((~fcst_bool)&(~rean_bool)).astype(np.int64).groupby('time.month').sum()\n",
    "            #caluclate Gilbert Skill Score. See my_tools.py for more documentation\n",
    "            GSS = my_tools.gilbert_skill_score(hits,miss,false_alarm,non_event)\n",
    "            #Write result to csv file\n",
    "            df_cold['day_{}'.format(lead_day)]=pd.Series(GSS,index=np.arange(1,13))\n",
    "df_warm.to_csv(path+'GSS/GSS_general_warm_lon_{}_{}_lat_{}_{}_{}.csv'.format(lon_0,lon_1,lat_0,lat_1,fcst_model))\n",
    "df_cold.to_csv(path+'GSS/GSS_general_cold_lon_{}_{}_lat_{}_{}_{}.csv'.format(lon_0,lon_1,lat_0,lat_1,fcst_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific heat waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all heat extremes\n",
    "file_pers_hw = 'persistent_heatwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "file_pers_cw = 'persistent_coldwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "file_short_hw = 'short_heatwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "file_short_cw = 'short_coldwaves_lon_{}_{}_lat_{}_{}.npy'.format(lon_0,lon_1,lat_1,lat_0)\n",
    "pers_hw = np.load(path+file_pers_hw)\n",
    "pers_cw = np.load(path+file_pers_cw)\n",
    "short_hw = np.load(path+file_short_hw)\n",
    "short_cw = np.load(path+file_short_cw)\n",
    "temp_extremez = [pers_hw,short_hw,pers_cw,short_cw]\n",
    "\n",
    "columnz = ['persistent_hw','short_hw','persistent_cw','short_cw',]\n",
    "index = pd.date_range(pd.Timestamp('1984-12-01'),pd.Timestamp('2019-11-30'))\n",
    "df = pd.DataFrame(index=index,columns=columnz,dtype=float)\n",
    "lead_dayz = [1,3,5,7,9]\n",
    "for lead_day in lead_dayz:\n",
    "    for j,temp_extreme in enumerate(temp_extremez):\n",
    "        column = columnz[j]\n",
    "        for i,date in enumerate(temp_extreme[:,0]):\n",
    "            if date<pd.Timestamp('1984-12-01')+pd.Timedelta(lead_day,'days'):\n",
    "                continue\n",
    "            begin_date = date - pd.Timedelta(lead_day,'days')\n",
    "            end_date = date + pd.Timedelta(9-lead_day,'days')\n",
    "            rean_sub = rean.sel(time=slice(begin_date,end_date)).load()\n",
    "            fcst_sub = fcst.sel(time=begin_date).load()\n",
    "            timez = [pd.Timestamp('2016-{:02d}-{:02d}'.format(i.month,i.day)) for i in pd.date_range(begin_date,end_date)]\n",
    "            if j<2:\n",
    "                clim_p90_sub = clim_p90.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t>clim_p90_sub.t.values\n",
    "                fcst_bool = fcst_sub.t>clim_p90_sub.t.values\n",
    "            else:\n",
    "                clim_p10_sub = clim_p10.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t<clim_p10_sub.t.values\n",
    "                fcst_bool = fcst_sub.t<clim_p10_sub.t.values\n",
    "            hits = (rean_bool & fcst_bool.values).astype(np.int64).sum()\n",
    "            miss = ((~fcst_bool)&(rean_bool.values)).astype(np.int64).sum()\n",
    "            false_alarm = ((fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()\n",
    "            non_event = ((~fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()            \n",
    "            GSS = my_tools.gilbert_skill_score(hits,miss,false_alarm,non_event)\n",
    "            df.loc[date][column]=GSS\n",
    "\n",
    "    df = df.dropna(how='all')\n",
    "    df.to_csv(path+'GSS/Gilbert_Skill_Score_lead_day_{}_lon_{}_{}_lat_{}_{}_{}.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                                           lat_0,lat_1,fcst_model))\n",
    "#     df_month = df.groupby(df.index.month,dropna=True).mean()\n",
    "#     df_month.to_csv(path+'GSS/Gilbert_Skill_Score_Monthly_Mean_lead_day_{}.csv'.format(lead_day))\n",
    "df_count = df.groupby(df.index.month,dropna=True).count()\n",
    "df_count.to_csv(path+'GSS/Temp_extremes_count_lon_{}_{}_lat_{}_{}.csv'.format(lon_0,lon_1,lat_0,lat_1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GSS based on monthly grouped contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnz = ['persistent_hw','short_hw','persistent_cw','short_cw',]\n",
    "index = pd.date_range(pd.Timestamp('1984-12-01'),pd.Timestamp('2019-11-30'))\n",
    "df = pd.DataFrame(index=np.arange(1,13),columns=columnz,dtype=float)\n",
    "lead_dayz = [1,3,5,7,9]\n",
    "for lead_day in lead_dayz:\n",
    "    for j,temp_extreme in enumerate(temp_extremez):\n",
    "        column = columnz[j]\n",
    "        df_contingency = pd.DataFrame(columns=['hits','miss','false_alarm','non_event'])\n",
    "        for i,date in enumerate(temp_extreme[:,0]):\n",
    "            if date<pd.Timestamp('1984-12-01')+pd.Timedelta(lead_day,'days'):\n",
    "                continue\n",
    "            begin_date = date - pd.Timedelta(lead_day,'days')\n",
    "            end_date = date + pd.Timedelta(9-lead_day,'days')\n",
    "            rean_sub = rean.sel(time=slice(begin_date,end_date)).load()\n",
    "            fcst_sub = fcst.sel(time=begin_date).load()\n",
    "            timez = [pd.Timestamp('2016-{:02d}-{:02d}'.format(i.month,i.day)) for i in pd.date_range(begin_date,end_date)]\n",
    "            if j<2:\n",
    "                clim_p90_sub = clim_p90.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t>clim_p90_sub.t.values\n",
    "                fcst_bool = fcst_sub.t>clim_p90_sub.t.values\n",
    "            else:\n",
    "                clim_p10_sub = clim_p10.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t<clim_p10_sub.t.values\n",
    "                fcst_bool = fcst_sub.t<clim_p10_sub.t.values\n",
    "            hits = (rean_bool & fcst_bool.values).astype(np.int64).sum()\n",
    "            miss = ((~fcst_bool)&(rean_bool.values)).astype(np.int64).sum()\n",
    "            false_alarm = ((fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()\n",
    "            non_event = ((~fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()    \n",
    "            df_contingency.loc[date] = [int(hits),int(miss),int(false_alarm),int(non_event)]\n",
    "\n",
    "        df_contingency_month = df_contingency.groupby(df_contingency.index.month).sum()\n",
    "        GSS = my_tools.gilbert_skill_score_dataframe(df_contingency_month)\n",
    "        df[column]=GSS\n",
    "    df.to_csv(path+'GSS/GSS_Temp_Extremes_Monthly_Grouped_lead_day_{}_lon_{}_{}_lat_{}_{}_{}.csv'.format(lead_day,lon_0,lon_1,\n",
    "                                                                                                         lat_0,lat_1,fcst_model))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GSS based on seasonally grouped contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lead Day is 1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onno/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "columnz = ['persistent_hw','persistent_cw']\n",
    "index = pd.date_range(pd.Timestamp('1984-12-01'),pd.Timestamp('2019-11-30'))\n",
    "df = pd.DataFrame(index=['DJF','JJA','MAM','SON'],columns=columnz,dtype=float)\n",
    "month_to_season_lu = np.array([\n",
    "    None,\n",
    "    'DJF', 'DJF',\n",
    "    'MAM', 'MAM', 'MAM',\n",
    "    'JJA', 'JJA', 'JJA',\n",
    "    'SON', 'SON', 'SON',\n",
    "    'DJF'\n",
    "])\n",
    "lead_dayz = [1,3,5,7,9]\n",
    "for lead_day in lead_dayz:\n",
    "    print(' Lead Day is {}'.format(lead_day))\n",
    "    for j,temp_extreme in enumerate(temp_extremez):\n",
    "        column = columnz[j]\n",
    "        df_contingency = pd.DataFrame(columns=['hits','miss','false_alarm','non_event'])\n",
    "        for i,date in enumerate(temp_extreme[:,0]):\n",
    "            date = pd.Timestamp(date)\n",
    "            if date<pd.Timestamp('1984-12-01')+pd.Timedelta(lead_day,'days'):\n",
    "                continue\n",
    "            begin_date = date - pd.Timedelta(lead_day,'days')\n",
    "            end_date = date + pd.Timedelta(9-lead_day,'days')\n",
    "            rean_sub = rean.sel(time=slice(begin_date,end_date)).load()\n",
    "            fcst_sub = fcst.sel(time=begin_date).load()\n",
    "            timez = [pd.Timestamp('2016-{:02d}-{:02d}'.format(i.month,i.day)) for i in pd.date_range(begin_date,end_date)]\n",
    "            if j==0:\n",
    "                clim_p90_sub = clim_p90.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t>clim_p90_sub.t.values\n",
    "                fcst_bool = fcst_sub.t>clim_p90_sub.t.values\n",
    "            else:\n",
    "                clim_p10_sub = clim_p10.sel(time=timez).load()\n",
    "                rean_bool = rean_sub.t<clim_p10_sub.t.values\n",
    "                fcst_bool = fcst_sub.t<clim_p10_sub.t.values\n",
    "            hits = (rean_bool & fcst_bool.values).astype(np.int64).sum()\n",
    "            miss = ((~fcst_bool)&(rean_bool.values)).astype(np.int64).sum()\n",
    "            false_alarm = ((fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()\n",
    "            non_event = ((~fcst_bool)&(~rean_bool.values)).astype(np.int64).sum()    \n",
    "            df_contingency.loc[date] = [int(hits),int(miss),int(false_alarm),int(non_event)]\n",
    "        if (lead_day==9)&(j==0):\n",
    "            sys.exit()\n",
    "\n",
    "        df_contingency_month = df_contingency.groupby(month_to_season_lu[df_contingency.index.month]).sum()\n",
    "        GSS = my_tools.gilbert_skill_score_dataframe(df_contingency_month)\n",
    "        df[column]=GSS\n",
    "    sys.exit()\n",
    "    print(df)\n",
    "#     df.to_csv(path+'GSS/GSS_Temp_Extremes_Seasonally_Grouped_lead_day_{}.csv'.format(lead_day))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onno/miniconda3/envs/thesis/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "lead_dayz = [1,3,5,7,9]\n",
    "file = 'GSS/Gilbert_Skill_Score_lead_day_{}.csv'\n",
    "month_to_season_lu = np.array([\n",
    "    None,\n",
    "    'DJF', 'DJF',\n",
    "    'MAM', 'MAM', 'MAM',\n",
    "    'JJA', 'JJA', 'JJA',\n",
    "    'SON', 'SON', 'SON',\n",
    "    'DJF'\n",
    "])\n",
    "for lead_day in lead_dayz:\n",
    "    df = pd.read_csv(path+file.format(lead_day),index_col=0)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df_seasonally = df.groupby(month_to_season_lu[df.index.month]).mean()\n",
    "    sys.exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
